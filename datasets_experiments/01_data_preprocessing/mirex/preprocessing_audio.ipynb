{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Standard and Melody-Based Audio Features\n",
    "\n",
    "In this notebook, both standard and melody-related audio features are extracted. Standard audio features are obtained using the `pyAudioAnalysis` package, and a comprehensive list of these features can be found [here](https://github.com/tyiannak/pyAudioAnalysis/wiki/3.-Feature-Extraction).\n",
    "\n",
    "Melody features are extracted using the `Melodia` plug-in developed by Justin Salamon. More details about `Melodia` can be found on the [website](https://www.upf.edu/web/mtg/melodia). The extraction of the features is performed based on the paper by Salamon et al., which is available [here](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamonrochagomezicassp2012.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gosia/miniconda3/envs/data_music/lib/python3.12/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# librosa\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# pyAudioAnalysis\n",
    "from pyAudioAnalysis import ShortTermFeatures as stF\n",
    "from pyAudioAnalysis import MidTermFeatures as mtF\n",
    "\n",
    "# to load Melodia plug-in\n",
    "import vamp\n",
    "\n",
    "# to play the audio files\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio\n",
    "\n",
    "# plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard features: pyAudioAnalysis and librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_standard_features(audio_file):\n",
    "    \n",
    "    data, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "    # librosa features\n",
    "    tempo_librosa, _ = librosa.beat.beat_track(y=data.astype('float'), sr=sr, units=\"time\")\n",
    "\n",
    "    # pyAudioAnalysis\n",
    "    win, step = 0.050, 0.050\n",
    "    [f, fn] = stF.feature_extraction(data, sr, int(sr * win), \n",
    "                                    int(sr * step))\n",
    "    mt, st, mt_n = mtF.mid_feature_extraction(data, sr, len(data), len(data), \n",
    "                                              0.05 * sr, 0.05 * sr)\n",
    "    tempo_pyAA, _ = mtF.beat_extraction(stF.feature_extraction(data, sr, int(sr * win), int(sr * step))[0], win)\n",
    "\n",
    "    df_mt = pd.DataFrame(mt.T)\n",
    "    df_mt.columns = [mt_n[i]+'_pyAA' for i in range(len(mt_n))]\n",
    "    df_mt['tempo_pyAA'] = tempo_pyAA\n",
    "\n",
    "\n",
    "    df = {'sr': [sr],\n",
    "          'samples': [len(data)],\n",
    "          'duration_sec': [len(data)/sr],\n",
    "          'tempo_librosa': [tempo_librosa]\n",
    "          }\n",
    "\n",
    "    df = pd.DataFrame.from_dict(df)\n",
    "    df = pd.concat([df, df_mt], axis=1, join=\"inner\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melody features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vibrato_stats(contour_hz):\n",
    "\n",
    "    # transform to cents\n",
    "    contour = 1200 * np.log2(contour_hz/55)\n",
    "\n",
    "    # hop\n",
    "    H = 128\n",
    "    # sampling rate\n",
    "    f_s = 44100\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "            'vibrato_length': [0],\n",
    "            'vibrato_coverage': [0],\n",
    "            'vibrato_rate_hz': [np.nan],\n",
    "            'vibrato_extend_cents': [np.nan],\n",
    "            'vibrato_extend_hz': [np.nan]\n",
    "    })\n",
    "\n",
    "    if contour_hz.shape[0] < 125:\n",
    "        return df\n",
    "\n",
    "    # window size\n",
    "    n_fft_contour = np.min([180, contour_hz.shape[0]-5])\n",
    "\n",
    "    pitches, magnitudes = librosa.piptrack(y=contour_hz, sr=f_s/H, n_fft=n_fft_contour, hop_length=1, ref=np.mean, fmin=1, center=True)\n",
    "    pitches[pitches==0] = np.nan\n",
    "    # range of human vibrato\n",
    "    vibrato_ind = ((1*(pitches >= 5) + 1*(pitches <= 8)) == 2)\n",
    "\n",
    "    if np.sum(np.any(vibrato_ind, axis=0)) > 0:\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'vibrato_length': [np.sum(np.any(vibrato_ind, axis=0))*H/f_s],\n",
    "            'vibrato_coverage': [np.sum(np.any(vibrato_ind, axis=0))/vibrato_ind.shape[1]],\n",
    "            'vibrato_rate_hz': [np.mean(pitches[vibrato_ind])],\n",
    "            'vibrato_extend_cents': [1200 * np.log2(np.mean(magnitudes[vibrato_ind])/55)],\n",
    "            'vibrato_extend_hz': [np.mean(magnitudes[vibrato_ind])]\n",
    "        })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def contour_extract_features(contour_hz):\n",
    "\n",
    "    # transform to cents\n",
    "    contour = 1200 * np.log2(contour_hz/55)\n",
    "\n",
    "    # hop\n",
    "    H = 128\n",
    "    # sampling rate\n",
    "    f_s = 44100\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'duration': [(contour).shape[0]*H/f_s],\n",
    "        'pitch_mean_height_cents': [np.mean(contour)],\n",
    "        'pitch_deviation_cents': [np.std(contour)],\n",
    "        'pitch_range_cents': [np.max(contour) - np.min(contour)],\n",
    "        'pitch_min_cents': [np.min(contour)],\n",
    "        'pitch_max_cents': [np.max(contour)],\n",
    "        'pitch_mean_height_hz': [np.mean(contour_hz)],\n",
    "        'pitch_deviation_hz': [np.std(contour_hz)],\n",
    "        'pitch_range_hz': [np.max(contour_hz) - np.min(contour_hz)],\n",
    "        'pitch_min_hz': [np.min(contour_hz)],\n",
    "        'pitch_max_hz': [np.max(contour_hz)]\n",
    "    })\n",
    "\n",
    "    df_vibrato = vibrato_stats(contour_hz)\n",
    "    df = pd.concat([df, df_vibrato], axis=1, join=\"inner\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def song_contour_extract_melody_features(melody_hz):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    melody_hz_tmp = np.hstack((melody_hz, np.nan))\n",
    "    \n",
    "    start_ind = np.where(~np.isnan(melody_hz_tmp))[0]\n",
    "\n",
    "    interval_length = []\n",
    "\n",
    "    while start_ind.shape[0] > 0:\n",
    "        interval_length.append(start_ind[0])\n",
    "        melody_hz_tmp = melody_hz_tmp[start_ind[0]:]\n",
    "        end_ind = np.where(np.isnan(melody_hz_tmp))[0]\n",
    "\n",
    "        contour_hz_tmp = melody_hz_tmp[:end_ind[0]]\n",
    "\n",
    "        if contour_hz_tmp.shape[0] > 50:\n",
    "            df_tmp = contour_extract_features(contour_hz_tmp)\n",
    "            df = pd.concat([df, df_tmp], ignore_index = True)\n",
    "\n",
    "        melody_hz_tmp = melody_hz_tmp[(end_ind[0]+1):]\n",
    "        start_ind = np.where(~np.isnan(melody_hz_tmp))[0]\n",
    "\n",
    "    interval_length = interval_length[1:]\n",
    "    \n",
    "    return df, interval_length\n",
    "\n",
    "\n",
    "def melody_features_from_df(df_all):\n",
    "\n",
    "    df_tmp = pd.concat([df_all.mean(),\n",
    "                        df_all.std()])\n",
    "    df_tmp = pd.DataFrame(df_tmp).T\n",
    "\n",
    "    df_tmp.columns = [df_all.columns[i] +'_mean' for i in range(len(df_all.columns))] + \\\n",
    "        [df_all.columns[i] +'_std' for i in range(len(df_all.columns))] \n",
    "    \n",
    "    df_global = pd.DataFrame({\n",
    "        'global_highest_pitch_cents': [np.max(df_all['pitch_max_cents'])],\n",
    "        'global_lowest_pitch_cents': [np.min(df_all['pitch_min_cents'])],\n",
    "        'global_pitch_range_cents': [np.max(df_all['pitch_max_cents']) - np.min(df_all['pitch_min_cents'])],\n",
    "        'global_vibrato_coverage': [df_all['vibrato_length'].sum()/df_all['duration'].sum()]\n",
    "    })\n",
    "\n",
    "    df = pd.concat([df_global, df_tmp], axis=1, join=\"inner\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_melody_features(audio_file):\n",
    "\n",
    "    data, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "    data_melodia_melody = vamp.collect(data, sr, \"mtg-melodia:melodia\", output='melody')\n",
    "    hop, melody_hz = data_melodia_melody['vector']\n",
    "    melody_hz[melody_hz<=0] = np.nan\n",
    "    melody = 1200 * np.log2(melody_hz/55)\n",
    "\n",
    "    df_all, interval_lengths = song_contour_extract_melody_features(melody_hz)\n",
    "\n",
    "    df100 = melody_features_from_df(df_all)\n",
    "    df100.columns = [df100.columns[i] + '_all' for i in range(len(df100.columns))]\n",
    "    df100['interval_duration_mean_all'] = np.mean(interval_lengths)\n",
    "    df100['interval_duration_std_all'] = np.std(interval_lengths)\n",
    "\n",
    "    # observation with duration above q_1/3 (2/3 of observations chosen)\n",
    "    toptwothird_duration = (df_all['duration'] >= np.array(df_all['duration'].sort_values())[int(np.floor(len(df_all)*1/3))])\n",
    "    df_all_twothird = df_all.loc[toptwothird_duration,:]\n",
    "    df_twothird = melody_features_from_df(df_all_twothird)\n",
    "    df_twothird.columns = [df_twothird.columns[i] + '_twothird' for i in range(len(df_twothird.columns))]\n",
    "\n",
    "    # observation with duration above q_2/3 (1/3 of observations chosen)\n",
    "    toponethird_duration = (df_all['duration'] >= np.array(df_all['duration'].sort_values())[int(np.floor(len(df_all)*2/3))])\n",
    "    df_all_onethird = df_all.loc[toponethird_duration,:]\n",
    "    df_onethird = melody_features_from_df(df_all_onethird)\n",
    "    df_onethird.columns = [df_onethird.columns[i] + '_onethird' for i in range(len(df_onethird.columns))]\n",
    "\n",
    "    df = pd.concat([df100, df_twothird], axis=1, join=\"inner\")\n",
    "    df = pd.concat([df, df_onethird], axis=1, join=\"inner\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features for all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_paths):\n",
    "\n",
    "    # standard features\n",
    "    df_SF = pd.DataFrame()\n",
    "    # melody features\n",
    "    df_MF = pd.DataFrame()\n",
    "\n",
    "    for file_path in tqdm(file_paths):\n",
    "        df_SF_tmp = extract_standard_features(file_path)\n",
    "        df_SF_tmp[['file_path']] = file_path\n",
    "        df_SF_tmp[['song_id']] = re.findall('[0-9]+', file_path)[0]\n",
    "        df_SF = pd.concat([df_SF, df_SF_tmp], ignore_index = True)\n",
    "\n",
    "        df_MF_tmp = extract_melody_features(file_path)\n",
    "        df_MF_tmp[['file_path']] = file_path\n",
    "        df_MF_tmp[['song_id']] = re.findall('[0-9]+', file_path)[0]\n",
    "        df_MF = pd.concat([df_MF, df_MF_tmp], ignore_index = True)\n",
    "\n",
    "    return df_SF, df_MF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = 'audio_path'\n",
    "music_directory = os.listdir(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "for file_name in music_directory:\n",
    "    # Create the full file path using os.path.join()\n",
    "    file_path = os.path.join(audio, file_name)\n",
    "    \n",
    "    # Add the file path to the list\n",
    "    file_paths.append(file_path)\n",
    "    \n",
    "    \n",
    "def extract_numeric_part(file_path):\n",
    "    return [int(s) for s in os.path.basename(file_path).split('.') if s.isdigit()][0]\n",
    "\n",
    "# Sort the file paths based on the numeric values in the file names\n",
    "file_paths = sorted(file_paths, key=extract_numeric_part)\n",
    "# Delete 'zone identifier':\n",
    "file_paths = [file_path for file_path in file_paths if not 'Zone.Identifier' in file_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 903/903 [1:26:10<00:00,  5.73s/it]\n"
     ]
    }
   ],
   "source": [
    "df_SF, df_MF = extract_features(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SF.to_csv('data/audio/preprocessed_SF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MF.to_csv('data/audio/preprocessed_MF.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
