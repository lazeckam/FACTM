{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Standard Text Features\n",
    "\n",
    "We extract here features from lyrics for a simple view and for a structered view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import dill\n",
    "from tqdm import tqdm\n",
    "\n",
    "# tokenization of lyrics\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp_blob = spacy.load(\"en_core_web_trf\")\n",
    "nlp_blob.add_pipe('spacytextblob')\n",
    "\n",
    "# for a structered view\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# to get sentiment of individual words\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_lyrics = ''\n",
    "path_to_save = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = path_to_lyrics\n",
    "lyrics_directory = os.listdir(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths_lyrics = []\n",
    "for file_name in lyrics_directory:\n",
    "    # Create the full file path using os.path.join()\n",
    "    file_path = os.path.join(lyrics, file_name)\n",
    "    \n",
    "    # Add the file path to the list\n",
    "    file_paths_lyrics.append(file_path)\n",
    "    \n",
    "    \n",
    "def extract_numeric_part(file_path):\n",
    "    return [int(s) for s in os.path.basename(file_path).split('.') if s.isdigit()][0]\n",
    "\n",
    "file_paths_lyrics = sorted(file_paths_lyrics, key=extract_numeric_part)\n",
    "file_paths_lyrics = [file_path for file_path in file_paths_lyrics if not 'Zone.Identifier' in file_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of all instances of the word 'chorus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chorus = []\n",
    "for file_path in file_paths_lyrics:\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            lyrics_line = line.strip('\\n')\n",
    "            if \"chorus\" in (lyrics_line).lower():\n",
    "                chorus_tmp = lyrics_line.replace('[', '\\\\[')\n",
    "                chorus_tmp = chorus_tmp.replace(']', '\\\\]')\n",
    "                chorus_tmp = chorus_tmp.replace('*', '\\\\*')\n",
    "                chorus_tmp = chorus_tmp.replace('(', '\\\\(')\n",
    "                chorus_tmp = chorus_tmp.replace(')', '\\\\)')\n",
    "                chorus.append(chorus_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chorus = np.unique(chorus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chorus[6] = 'Chorus: '\n",
    "chorus[7] = 'Chorus: ' \n",
    "chorus[8] = 'Chorus: '\n",
    "chorus[11] = 'Chorus: '\n",
    "chorus[23] = '\\\\*chorus starts\\\\* '\n",
    "chorus[38] = '\\\\[Chorus\\\\] '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chorus = [chorus[i].lower() for i in range(len(chorus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chorus = np.unique(chorus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of [repeat] and other comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetition_styles = ['[(\\\\[]*sing [0-9]+x[)\\\\]]*', '[(\\\\[]*sing x[0-9]+[)\\\\]]*',\n",
    "                     '[(\\\\[]*repeat [0-9]+x[)\\\\]]*', '[(\\\\[]*repeat x[0-9]+[)\\\\]]*',\n",
    "                     '[(\\\\[]*[0-9]+x[)\\\\]]*','[(\\\\[]*x[0-9]+[)\\\\]]*',\n",
    "                     '[(\\\\[]+.*repeat.*[)\\\\]]+',\n",
    "                     '\\\\(scat singing + title, x4\\\\)',\n",
    "                     '\\\\(repeat to fade',\n",
    "                     'coda \\\\[repeat to fade over trash organ riff from intro\\\\]: ',\n",
    "                     'repeat \\\\(\\\\*\\\\)',\n",
    "                     'repeat 3 times',\n",
    "                     'repeat to fade',\n",
    "\n",
    "                     '[(\\\\[]+.*solo.*[)\\\\]]+',\n",
    "                     '[(\\\\[]+.*break.*[)\\\\]]+',\n",
    "                     '[(\\\\[]+.*instrumental.*[)\\\\]]+',\n",
    "                     '[(\\\\[]+.*spoken.*[)\\\\]]+',\n",
    "                     '[(\\\\[]+.*guitar.*[)\\\\]]+',\n",
    "                     '[(\\\\[]+.*whistle.*[)\\\\]]+',\n",
    "                     '[(\\\\[]+.*noises.*[)\\\\]]+',\n",
    "                     '[(\\\\[]+.*chanting.*[)\\\\]]+',\n",
    "                     '[(\\\\[]+.*lyrics.*[)\\\\]]+',\n",
    "                     '[(\\\\[]+.*verse.*[)\\\\]]+',\n",
    "                     '[(\\\\[]+.*outro.*[)\\\\]]+',\n",
    "                     '[(\\\\[]+.*interlude.*[)\\\\]]+',\n",
    "                     '\\\\[Hook\\\\]', '\\\\[DMX\\\\]', '\\\\[LL\\\\]',\n",
    "                     '[(\\\\[]+', '[)\\\\]]+'\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset\n",
    "\n",
    "Delete 'chorus' and 'reapeat' etc, delete stop words\n",
    "\n",
    "Split into lines and verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_the_longest_matching(text, substrings):\n",
    "    all_matching = [re.findall(substrings[i], text)[0] for i in range(len(substrings)) if re.search(substrings[i], text) != None]\n",
    "    if len(all_matching) == 0:\n",
    "        return text, False\n",
    "    len_all_matching = [len(all_matching[i]) for i in range(len(all_matching))]\n",
    "    the_longest_matching = np.argmax(len_all_matching)\n",
    "    return text.replace(all_matching[the_longest_matching], ''), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_part_of_speech = ['DET', 'PRON', 'CONJ', 'CCONJ', 'ADP', 'PUNCT']\n",
    "\n",
    "def one_song_lyrics(file_path):\n",
    "    \n",
    "    lyrics = []\n",
    "    lyrics_tokenization = []\n",
    "    verse_ind = []\n",
    "    line_ind = []\n",
    "    verse_num = 0\n",
    "    line_num = 0\n",
    "    prev_verse_num = False\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            lyrics_line = line.strip('\\n').lower()\n",
    "\n",
    "            lyrics_line, is_chorus = delete_the_longest_matching(lyrics_line, chorus)\n",
    "            lyrics_line, _ = delete_the_longest_matching(lyrics_line, repetition_styles)\n",
    "            \n",
    "            if (len(lyrics_line.lower()) == 0 or lyrics_line.lower() == '\\n'  or lyrics_line.lower() == ' ' or is_chorus):\n",
    "                if not prev_verse_num:\n",
    "                    verse_num += 1\n",
    "                    prev_verse_num = True\n",
    "            else:\n",
    "                prev_verse_num = False\n",
    "                verse_ind.append(verse_num)\n",
    "                line_ind.append(line_num)\n",
    "                line_num += 1\n",
    "                lyrics.append(lyrics_line)\n",
    "                nlp_n = nlp(lyrics_line)\n",
    "                nlp_n = [nlp_n_w for nlp_n_w in nlp_n if not nlp_n_w.pos_ in stop_words_part_of_speech]\n",
    "                lyrics_tokenization.append(' '.join([nlp_n_w.lemma_ for nlp_n_w in nlp_n]))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'verse': verse_ind,\n",
    "        'line': line_ind,\n",
    "        'lyrics': lyrics,\n",
    "        'tokenized_lyrics': lyrics_tokenization\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_paths_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 764/764 [17:55<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "df = None\n",
    "for file_path in tqdm(file_paths_lyrics):\n",
    "    df_tmp = one_song_lyrics(file_path)\n",
    "    df_tmp[['file_path']] = file_path\n",
    "    df_tmp[['song_id']] = re.findall('[0-9]+', file_path)\n",
    "    df = pd.concat([df_tmp, df])\n",
    "    \n",
    "df = df[::-1]\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/text_modality/preprocessed_lyrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features for a simple view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(path_to_save,'preprocessed_lyrics.csv'), dtype=str, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "\n",
    "    song_ids = np.unique(df['song_id'])\n",
    "    df_res=pd.DataFrame()\n",
    "\n",
    "    for song_num in tqdm(range(len(song_ids))):\n",
    "        df_tmp = df[df['song_id'] == song_ids[song_num]]\n",
    "        tokenized_lyrics_join = ' '.join(df_tmp['tokenized_lyrics'].dropna())\n",
    "        tokenized_lyrics_join_split = tokenized_lyrics_join.split(' ')\n",
    "        word_token_len = [len(tokenized_lyrics_join_split[i]) for i in range(len(tokenized_lyrics_join_split))]\n",
    "        lyrics_join = ' '.join(df_tmp['lyrics'])\n",
    "        lyrics_join_split = lyrics_join.split(' ')\n",
    "\n",
    "        line_num = len(df_tmp['lyrics'])\n",
    "        word_num = len(lyrics_join_split)\n",
    "        verse_num = np.unique(df_tmp['verse']).shape[0]\n",
    "        word_token_num = len(tokenized_lyrics_join_split)\n",
    "        word_token_unique_num = len(np.unique(tokenized_lyrics_join_split))\n",
    "        word_token_mean_len = np.mean(word_token_len)\n",
    "        word_token_max_len = np.max(word_token_len)\n",
    "        word_token_std_len = np.std(word_token_len)\n",
    "\n",
    "        stop_words_part_of_speech = ['DET', 'PRON', 'CONJ', 'CCONJ', 'ADP', 'PUNCT']\n",
    "        nlp_n = nlp_blob(lyrics_join)\n",
    "        stop_words_song = [nlp_n_w.pos_ for nlp_n_w in nlp_n if nlp_n_w.pos_ in stop_words_part_of_speech]\n",
    "        stop_words_counts = [stop_words_song.count(stop_words_part_of_speech[i]) for i in range(len(stop_words_part_of_speech))]\n",
    "\n",
    "        polarity = nlp_n._.blob.polarity\n",
    "        subjectivity = nlp_n._.blob.subjectivity\n",
    "\n",
    "        df_res_tmp = pd.DataFrame({\n",
    "            'lines': [line_num],\n",
    "            'words': [word_num],\n",
    "            'verses': [verse_num],\n",
    "            'words_token': [word_token_num],\n",
    "            'unique_words_token': [word_token_unique_num],\n",
    "            'length_mean_words_token': [word_token_mean_len],\n",
    "            'length_max_words_token': [word_token_max_len],\n",
    "            'length_mstd_words_token': [word_token_std_len],\n",
    "            'polarity': polarity,\n",
    "            'subjectivity': subjectivity,\n",
    "            'DET': stop_words_counts[0],\n",
    "            'PRON': stop_words_counts[1],\n",
    "            'CONJ': stop_words_counts[2],\n",
    "            'CCONJ': stop_words_counts[3],\n",
    "            'ADP': stop_words_counts[4],\n",
    "            'PUNCT': stop_words_counts[5]\n",
    "        })\n",
    "\n",
    "        df_res = pd.concat([df_res, df_res_tmp], ignore_index = True)\n",
    "    \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 764/764 [07:51<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "df_res = extract_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_csv(os.path.join(path_to_save,'preprocessed_lyrics_simpleview.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare structered view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(path_to_save,'preprocessed_lyrics.csv'), dtype=str, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_nan = df['tokenized_lyrics'].isnull().values\n",
    "df = df.loc[~ind_nan,:]\n",
    "tokenized_lyrics = df['tokenized_lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of words that appeared at least 20 times\n",
    "min_df0 = 20\n",
    "vectorizer = CountVectorizer(min_df=min_df0, stop_words='english')\n",
    "X = vectorizer.fit_transform(tokenized_lyrics)\n",
    "dictionary_text_modality = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26287, 577)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 577 unique words\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocabulary - 577 words\n",
    "np.savetxt(os.path.join(path_to_save,'vocabulary_577.txt'), dictionary_text_modality, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ids of songs with lyrics\n",
    "song_ids = [str(i)[1:4] for i in range(1001, 1904)]\n",
    "df_is_lyrics = {\n",
    "    'song_id': song_ids,\n",
    "    'exists': [song_ids[i] in np.sort(df['song_id'].unique()) for i in range(len(song_ids))]\n",
    "}\n",
    "pd.DataFrame(df_is_lyrics).to_csv(os.path.join(path_to_save,'lyrics_exist.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data points (sentences) consist of words in lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "structered_view = [X[df['song_id'] == song_ids[i]].toarray() for i in range(len(song_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete empty samples\n",
    "structered_view = [structered_view[i] for i in range(len(structered_view)) if structered_view[i].shape[0] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete empty data points\n",
    "structered_view = [structered_view[i][np.sum(structered_view[i], axis=1) > 0,:] for i in range(len(structered_view))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chceck how many samples with small numbers of data points\n",
    "np.sum(np.array([structered_view[i].shape[0] for i in range(len(structered_view))]) < 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22564, 577)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of data points\n",
    "np.vstack(structered_view).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_save,'sentences_lines_577.pkl'), 'wb') as f:  \n",
    "    dill.dump(structered_view, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary - sentiment of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(os.path.join(path_to_save,'vocabulary_577.txt'), header=None)\n",
    "vocab = np.array(vocab[[0]])\n",
    "vocab = vocab.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sentiment of individual words\n",
    "neg = [sid.polarity_scores(vocab[i])['neg'] for i in range(len(vocab))]\n",
    "pos = [sid.polarity_scores(vocab[i])['pos'] for i in range(len(vocab))]\n",
    "com = [sid.polarity_scores(vocab[i])['compound'] for i in range(len(vocab))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.DataFrame({'vocab': vocab,\n",
    "                             'negative': neg,\n",
    "                             'positive': pos,\n",
    "                             'compound': com})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get part of speech of individual words\n",
    "part_of_speech = [nlp_blob(vocab[i])[0].pos_ for i in range(len(vocab))]\n",
    "df_pos = pd.get_dummies(part_of_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and save\n",
    "df_features = pd.concat([df_sentiment, df_pos], axis=1)\n",
    "df_features.to_csv(os.path.join(path_to_save,'lyrics_vocab_description_577.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
