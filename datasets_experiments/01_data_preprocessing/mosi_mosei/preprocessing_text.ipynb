{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text Modality\n",
    "\n",
    "In this section, we begin with the text modality data, which has been converted from GloVe embeddings back to words (`text_modality.pkl`, see `preprocessing_transform_from_glove_to_words.ipynb`). We then preprocess the text in the following way-:\n",
    "\n",
    "- Tokenization: Using the [spaCy](https://pypi.org/project/spacy/) package, we tokenize each text into individual words.\n",
    "- Stop word removal: Common stop words are removed.\n",
    "- Rare word filtering: Rare words are removed to reduce sparsity in the dataset.\n",
    "- Word counts by sentence: Finally, the data is transformed into word count representations, segmented by sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_input(how_long_sentences):\n",
    "\n",
    "    index_text_sentence = []\n",
    "    text_modality_sentence = [] \n",
    "\n",
    "    for i in tqdm(range(N)):\n",
    "        text_tmp = np.array([word for word in text_modality_token[i].split() if word not in (my_stops) and word in word_dictionary_vectorizer])\n",
    "        how_many_tmp = np.ceil(text_tmp.shape[0]/how_long_sentences).astype('int')\n",
    "\n",
    "        for j in range(how_many_tmp):\n",
    "            index_text_sentence.append(i)\n",
    "\n",
    "            if j == (how_many_tmp-1):\n",
    "                text_modality_sentence.append(' '.join(text_tmp[how_long_sentences*(how_many_tmp-1):]))\n",
    "            else:\n",
    "                text_modality_sentence.append(' '.join(text_tmp[(how_long_sentences*j):(how_long_sentences*j+how_long_sentences)]))\n",
    "\n",
    "    print('How many observations in, how many sentences', len(np.unique(np.array(index_text_sentence)))/N, len(index_text_sentence))\n",
    "    \n",
    "    vectorizer=CountVectorizer(min_df=min_df0)\n",
    "    X = vectorizer.fit_transform(text_modality_sentence)\n",
    "    word_dictionary = vectorizer.get_feature_names()\n",
    "    X = X.toarray()\n",
    "\n",
    "    data_text = []\n",
    "    text_not_empty_ind = []\n",
    "    for i in tqdm(range(N)):\n",
    "\n",
    "        if np.sum(X[np.array(index_text_sentence) == i,:]) > 0:\n",
    "\n",
    "            X_tmp = X[np.array(index_text_sentence) == i,:]\n",
    "            X_tmp = X_tmp[np.sum(X_tmp, axis=1) > 0, :]\n",
    "            data_text.append(X_tmp)\n",
    "            text_not_empty_ind.append(True)\n",
    "        \n",
    "        else: \n",
    "            data_text.append([])\n",
    "            text_not_empty_ind.append(False)\n",
    "\n",
    "    return data_text, text_not_empty_ind, index_text_sentence, text_modality_sentence, X, word_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_transformed/MOSI/text_modality.pkl', \"rb\") as input_file:\n",
    "    text_modality = pickle.load(input_file)\n",
    "N = len(text_modality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "We tokenize observed sentences and delete stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ashutoshtripathi.com/2020/04/13/parts-of-speech-tagging-and-dependency-parsing-using-spacy-nlp/\n",
    "stop_words_part_of_speech = ['DET', 'PRON', 'CONJ', 'CCONJ', 'ADP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_modality_token = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in tqdm(range(N)):\n",
    "    nlp_n = nlp(text_modality[n])\n",
    "    nlp_n = [nlp_n_w for nlp_n_w in nlp_n if not nlp_n_w.pos_ in stop_words_part_of_speech]\n",
    "    text_modality_token.append(' '.join([nlp_n_w.lemma_ for nlp_n_w in nlp_n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df0 = 5\n",
    "vectorizer = CountVectorizer(min_df=min_df0)\n",
    "X = vectorizer.fit_transform(text_modality_token)\n",
    "word_dictionary_vectorizer = vectorizer.get_feature_names()\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stops = ['be', 'do', 'have']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text, text_not_empty_ind, index_text_sentence, text_modality_sentence, X, word_dictionary = transform_to_input(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data_transformed/MOSI/text_modality_array_sentence1word.pkl', 'wb') as f:  \n",
    "#     pickle.dump([data_text, text_not_empty_ind], f, protocol=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOSEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_transformed/MOSEI/text_modality_v7619.pkl', \"rb\") as input_file:\n",
    "    text_modality_0 = pickle.load(input_file)\n",
    "text_modality_0 = text_modality_0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_transformed/MOSEI/text_modality_v15239.pkl', \"rb\") as input_file:\n",
    "    text_modality_1 = pickle.load(input_file)\n",
    "text_modality_1 = text_modality_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_transformed/MOSEI/text_modality_v22859.pkl', \"rb\") as input_file:\n",
    "    text_modality_2 = pickle.load(input_file)\n",
    "text_modality_2 = text_modality_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_modality = text_modality_0 + text_modality_1 + text_modality_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(text_modality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computed in `preprocessing_text_modality_MOSEI.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_transformed/MOSEI/text_modality_token_v0.pkl', \"rb\") as input_file:\n",
    "    text_modality_0 = pickle.load(input_file)\n",
    "with open('data_transformed/MOSEI/text_modality_token_v1.pkl', \"rb\") as input_file:\n",
    "    text_modality_1 = pickle.load(input_file)\n",
    "with open('data_transformed/MOSEI/text_modality_token_v2.pkl', \"rb\") as input_file:\n",
    "    text_modality_2 = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_modality_token = text_modality_0 + text_modality_1 + text_modality_2\n",
    "N = len(text_modality_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in tqdm(range(N)):\n",
    "    text_modality_token[n] = (' ').join([w for w in text_modality_token[n].split() if not 'youngentrepreneur.com' in w])\n",
    "    text_modality_token[n] = (' ').join([w for w in text_modality_token[n].split() if not 'ï¿½' in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df0 = 50\n",
    "vectorizer = CountVectorizer(min_df=min_df0)\n",
    "X = vectorizer.fit_transform(text_modality_token)\n",
    "word_dictionary_vectorizer = vectorizer.get_feature_names()\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stops = ['be', 'do', 'have']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text, text_not_empty_ind, index_text_sentence, text_modality_sentence, X, word_dictionary = transform_to_input(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_transformed/MOSEI/text_modality_array_sentence1words.pkl', 'wb') as f:  \n",
    "    pickle.dump([data_text, text_not_empty_ind], f, protocol=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_multibench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
