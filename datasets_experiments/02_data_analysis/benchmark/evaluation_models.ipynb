{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "import dill\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, auc, precision_recall_curve\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pr_auc(y_true, y_score):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
    "    auc_value = auc(recall, precision)\n",
    "    return auc_value\n",
    "\n",
    "def eval(factor_list, classes, kfolds_no=k_folds):\n",
    "\n",
    "    roc_auc = np.zeros((len(factor_list), kfolds_no))\n",
    "    pr_auc = np.zeros((len(factor_list), kfolds_no))\n",
    "    f1 = np.zeros((len(factor_list), kfolds_no))\n",
    "\n",
    "    np.random.seed(123)\n",
    "    cv = StratifiedKFold(kfolds_no, random_state=123, shuffle=True)  \n",
    "\n",
    "    for i in range(len(factor_list)):\n",
    "\n",
    "        X = factor_list[i]\n",
    "        y = classes\n",
    "        roc_auc_tmp = []\n",
    "        pr_auc_tmp = []\n",
    "        f1_tmp = []\n",
    "        i0 = 0\n",
    "        for train_index, test_index in cv.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            i0 += 1\n",
    "\n",
    "            mod = RandomForestClassifier(n_estimators=100, random_state=123+i0)\n",
    "            mod.fit(X_train, y_train)\n",
    "            y_prob = mod.predict_proba(X_test)[:,1]\n",
    "            roc_auc_tmp.append(roc_auc_score(y_test, y_prob))\n",
    "            pr_auc_tmp.append(compute_pr_auc(y_test, y_prob))\n",
    "            f1_tmp.append(f1_score(y_test, y_prob > 0.5))\n",
    "\n",
    "        roc_auc[i,:] = roc_auc_tmp\n",
    "        pr_auc[i,:] = pr_auc_tmp\n",
    "        f1[i,:] = f1_tmp\n",
    "\n",
    "\n",
    "    return roc_auc, pr_auc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_average(factor_list, list_of_names, classes, kfolds_no=k_folds):\n",
    "\n",
    "    classes_unique = np.unique(classes, return_counts=True)\n",
    "\n",
    "    classes_names = np.sort(classes_unique[0])\n",
    "\n",
    "    roc_auc = np.zeros((len(factor_list), kfolds_no))\n",
    "    pr_auc = np.zeros((len(factor_list), kfolds_no))\n",
    "    f1 = np.zeros((len(factor_list), kfolds_no))\n",
    "\n",
    "    roc_auc_all = np.zeros((classes_names.shape[0], len(factor_list), kfolds_no))\n",
    "    pr_auc_all = np.zeros((classes_names.shape[0], len(factor_list), kfolds_no))\n",
    "    f1_all = np.zeros((classes_names.shape[0], len(factor_list), kfolds_no))\n",
    "\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    for i in range(classes_names.shape[0]):\n",
    "        class_tmp = np.zeros(classes.shape[0])\n",
    "        class_tmp[classes == classes_names[i]] = 1\n",
    "        roc_auc_tmp, pr_auc_tmp, f1_tmp = eval(factor_list, class_tmp, kfolds_no)\n",
    "\n",
    "        for measure in ['roc_auc', 'pr_auc', 'f1']:\n",
    "            if measure == 'roc_auc':\n",
    "                perf = roc_auc_tmp\n",
    "            if measure == 'pr_auc':\n",
    "                perf = pr_auc_tmp\n",
    "            if measure == 'f1':\n",
    "                perf = f1_tmp\n",
    "            res_tmp = pd.DataFrame(perf.T)\n",
    "            res_tmp.columns = list_of_names\n",
    "            res_tmp = pd.melt(res_tmp, var_name='model', value_name='value')\n",
    "            res_tmp['measure'] = measure\n",
    "            res_tmp['class'] = classes_names[i]\n",
    "            df_all = pd.concat([df_all, res_tmp])\n",
    "            \n",
    "\n",
    "        roc_auc_all[i,:,:] = roc_auc_tmp\n",
    "        pr_auc_all[i,:,:] = pr_auc_tmp\n",
    "        f1_all[i,:,:] = f1_tmp\n",
    "\n",
    "        roc_auc += classes_unique[1][i]*roc_auc_tmp\n",
    "        pr_auc += classes_unique[1][i]*pr_auc_tmp\n",
    "        f1 += classes_unique[1][i]*f1_tmp\n",
    "\n",
    "    roc_auc = roc_auc/np.sum(classes_unique[1])\n",
    "    pr_auc = pr_auc/np.sum(classes_unique[1])\n",
    "    f1 = f1/np.sum(classes_unique[1])\n",
    "\n",
    "    return roc_auc, pr_auc, f1, df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_comparison_df(list_of_factors, list_of_names, classes):\n",
    "\n",
    "    classes_unique = np.unique(classes, return_counts=True)\n",
    "\n",
    "    classes_names = np.sort(classes_unique[0])\n",
    "\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    if classes_names.shape[0] == 2:\n",
    "        roc_auc, pr_auc, f1 = eval(list_of_factors, classes)\n",
    "    if classes_names.shape[0] > 2:\n",
    "        roc_auc, pr_auc, f1, df_all = eval_average(list_of_factors, list_of_names, classes)\n",
    "    \n",
    "    res = pd.DataFrame()\n",
    "\n",
    "    for measure in ['roc_auc', 'pr_auc', 'f1']:\n",
    "        if measure == 'roc_auc':\n",
    "            perf = roc_auc\n",
    "        if measure == 'pr_auc':\n",
    "            perf = pr_auc\n",
    "        if measure == 'f1':\n",
    "            perf = f1\n",
    "        res_tmp = pd.DataFrame(perf.T)\n",
    "        res_tmp.columns = list_of_names\n",
    "        res_tmp = pd.melt(res_tmp, var_name='model', value_name='value')\n",
    "        res_tmp['measure'] = measure\n",
    "        res = pd.concat([res, res_tmp])\n",
    "\n",
    "    return res, df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factors(file_list):\n",
    "    factor_list = []\n",
    "    for i in range(len(file_list)):\n",
    "        with open(file_list[i], \"rb\") as input_file:\n",
    "            factors, _, _ = dill.load(input_file)\n",
    "        factor_list.append(factors)\n",
    "    return factor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structered_representation(file_list, index_lda_log, index_lda=None):\n",
    "    factor_list = []\n",
    "    for i in range(len(file_list)):\n",
    "        with open(file_list[i], \"rb\") as input_file:\n",
    "            res = dill.load(input_file)\n",
    "        if i != index_lda_log and i != index_lda:\n",
    "            rep = res[3]\n",
    "        if i == index_lda_log:\n",
    "            rep = np.log(res[1])\n",
    "        if i == index_lda:\n",
    "            rep = res[1]\n",
    "        factor_list.append(rep)\n",
    "    return factor_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOSEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_transformed/mosei_preprocessed_data_sentence1words.pkl', \"rb\") as input_file:\n",
    "    factm_input = dill.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (factm_input['labels'] > 0) + 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['factm_fa_mosei.pkl',\n",
    "             'fa_ctm_mosei.pkl',\n",
    "             'mofa_mosei.pkl',\n",
    "             'muvi_mosei.pkl']\n",
    "model_list = ['factm', 'fa_ctm', 'mofa', 'muvi']\n",
    "factor_list = get_factors(file_list)\n",
    "results, _ = eval_comparison_df(factor_list, model_list, classes)\n",
    "results['dataset'] = 'mosei'\n",
    "# results.to_csv('mosei_factors_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['factm_ctm_mosei.pkl',\n",
    "              'ctm_mosei.pkl',\n",
    "              'lda_mosei.pkl']\n",
    "model_list = ['factm', 'ctm', 'lda']\n",
    "\n",
    "structered_representation_list = get_structered_representation(file_list, 2)\n",
    "results, _ = eval_comparison_df(structered_representation_list, model_list, classes)\n",
    "results['dataset'] = 'mosei'\n",
    "# results.to_csv('mosei_structured_classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_transformed/mosi_preprocessed_data_sentence1words.pkl', \"rb\") as input_file:\n",
    "    factm_input = dill.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (factm_input['labels'] > 0) + 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['factm_fa_mosi.pkl',\n",
    "             'fa_ctm_mosi.pkl',\n",
    "             'mofa_mosi.pkl',\n",
    "             'muvi_mosi.pkl']\n",
    "model_list = ['factm', 'fa_ctm', 'mofa', 'muvi']\n",
    "factor_list = get_factors(file_list)\n",
    "results, _ = eval_comparison_df(factor_list, model_list, classes)\n",
    "results['dataset'] = 'mosi'\n",
    "# results.to_csv('mosi_factors_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['factm_ctm_mosi.pkl',\n",
    "              'ctm_mosi.pkl',\n",
    "              'lda_mosi.pkl']\n",
    "model_list = ['factm', 'ctm', 'lda']\n",
    "\n",
    "structered_representation_list = get_structered_representation(file_list, 2)\n",
    "results, _ = eval_comparison_df(structered_representation_list, model_list, classes)\n",
    "results['dataset'] = 'mosi'\n",
    "# results.to_csv('mosi_structured_classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIREX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mirex_info.pkl', \"rb\") as input_file:\n",
    "    [feature_names, classes, categories, vocab] = dill.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['factm_fa_mirex.pkl',\n",
    "             'fa_ctm_mirex.pkl',\n",
    "             'mofa_mirex.pkl',\n",
    "             'muvi_mirex.pkl']\n",
    "model_list = ['factm', 'fa_ctm', 'mofa', 'muvi']\n",
    "factor_list = get_factors(file_list)\n",
    "results, results2 = eval_comparison_df(factor_list, model_list, classes)\n",
    "results2['dataset'] = 'mirex'\n",
    "# results2.to_csv('mirex_factors_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['factm_ctm_mirex.pkl',\n",
    "              'ctm_mirex.pkl',\n",
    "              'lda_mirex.pkl']\n",
    "model_list = ['factm', 'ctm', 'lda']\n",
    "\n",
    "structered_representation_list = get_structered_representation(file_list, 2)\n",
    "results, results2 = eval_comparison_df(structered_representation_list, model_list, classes)\n",
    "results2['dataset'] = 'mirex'\n",
    "# results2.to_csv('mirex_structured_classification.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FACTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
